[{"title":"Welcome to My Blog","url":"/2025/04/19/Welcome%20to%20My%20Blog/","content":"Hello and welcome to my personal blog! After much consideration, I finally decide to build my own blog as a base to record my process of sowing and growing.\nWhat to ExpectThis blog will be a collection of my ideas and learning experience during my study. Through this blog, I hope to not only document my own growth but also connect with others who have similar interests with me!\nA Little About This Space\nAcademic ideas and findings\nOccasional reflections on learning\nMy weekly summary of study\nUseful learning resources I discover along the way\n\nThank You for VisitingWhether you stumbled upon this blog by chance or came here intentionally, thank you for taking the time to read my first post. I look forward to sharing more content in the future and hopefully engaging in meaningful exchanges!Feel free to explore the blog and give comments. I’m excited about this new journey and appreciate your company along the way!\n"},{"title":"LLM Intro","url":"/2025/09/07/LLM-Intro/","content":"从自学说起此时是作者大三开学的第一周。\n在过去的两年大学生涯中，我通过自学学习了很多知识。从课内的数分、线代、模电和数电，到课外的深度学习、强化学习，一个比较常见的问题逐渐出现在主包的学习中，那就是遗忘。\n在大学的学习历程中，我们学习到的知识往往得不到高中那样做题式的巩固，而较早学习的知识如果长时间得不到使用，下一次见面可能就不是ex-xxfriend，而是complete stranger。\n落笔至此，已经是我第三次学习Transformer。因为作者在大学的研究方向是Federated Learning，而在我们领域内大部分都是基于一些小模型和小型CV数据集（MNIST, FMNIST, CIFAR-10）来做benchmark，所以对于大模型相关的知识用的很少，总是学了又忘。在FL做出一些成果之后，主包准备转向LLM来做一些新的research。借此机会，我也想记录下本人的学习历程和基于一些网络课程的常见Q&amp;A，希望自己能够坚持下去✍️\n本系列名为LLM-Intro，代表着成为LLM researcher的必经之路。\n参考资料：Happy-LLM\n目前计划内容如下（更新截止2025&#x2F;9&#x2F;7）[ ]  I: 从零开始搭建一个Transformer (Writing)[ ]  II: LLM的预训练 (Coming soon)\n","tags":["NLP","LLM"]}]
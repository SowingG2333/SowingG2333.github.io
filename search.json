[{"title":"Welcome to My Blog","url":"/2025/04/19/Welcome%20to%20My%20Blog/","content":"Hello and welcome to my personal blog! After much consideration, I finally decide to build my own blog as a base to record my process of sowing and growing.\nWhat to ExpectThis blog will be a collection of my ideas and learning experience during my study. Through this blog, I hope to not only document my own growth but also connect with others who have similar interests with me!\nA Little About This Space\nAcademic ideas and findings\nOccasional reflections on learning\nMy weekly summary of study\nUseful learning resources I discover along the way\n\nThank You for VisitingWhether you stumbled upon this blog by chance or came here intentionally, thank you for taking the time to read my first post. I look forward to sharing more content in the future and hopefully engaging in meaningful exchanges!Feel free to explore the blog and give comments. I’m excited about this new journey and appreciate your company along the way!\n"},{"title":"LLM-Tour | NLP-Base","url":"/2025/08/30/LLM-Tour-NLP-Base/","content":"发展历程\n早期探索：图灵测试 “如果一台机器可以通过使用打字机成为对话的一部分，并且能够完全模仿人类，没有明显的差异，那么机器可以被认为是能够思考的。”\n两大流派：符号主义和统计方法\n符号主义（symbolism）\n概括：知识表示（符号、规则和逻辑结构）+推理（采用预设规则进行分析）\n专家系统：核心即为“if-then”的推理规则\n句法分析器（synaptic parser）：基于CFG生成句法树（parser tree）  ![[Pasted image 20250830145629.png]]\n语义网络（semantic network）：采用图谱表示深层次的语义知识，节点代表概念，边代表联系\n\n\n&#x3D;&#x3D;统计方法&#x3D;&#x3D;\nHMMs（计算的是联合概率）：前向算法评估模型符合度、维特比算法寻找最佳路径、鲍姆-韦尔奇算法学习参数直至收敛（发射概率、状态转移概率和初始概率）\nCRFs（计算的是条件概率）：最大化训练数据的对数似然，梯度下降\nN-gram：第n个词语的生成依赖于前n-1个词语，基于markov\n传统ml算法：svm、决策树等\n\n\n\n\n机器学习和深度学习\n\nNLP任务\n中文分词（CWS）\n子词切分：适用于处理词汇稀疏问题，即当遇到罕见词或未见过的新词时，能够通过已知的子词单位来理解或生成这些词汇，在合成词多的语言以及预训练中尤为重要\n&#x3D;&#x3D;方法&#x3D;&#x3D;：Byte Pair Encoding (BPE)、WordPiece、Unigram、SentencePiece\n\n\n词性标注\n文本分类\n实体识别\n关系抽取\n文本摘要：抽取式和生成式\n机器翻译\n自动问答：检索式、知识库和社区问答\n\n文本表示\n词向量：VSM模型\n语言模型\nN-gram\n&#x3D;&#x3D;Word2Vec&#x3D;&#x3D;：利用词在文本中的上下文信息来捕捉词之间的语义关系，从而使得语义相似或相关的词在向量空间中距离较近\n连续词袋模型CBOW(Continuous Bag of Words)：根据目标词上下文中的词对应的词向量, 计算并输出目标词的向量表示\nSkip-Gram模型：利用目标词的向量表示计算上下文中的词向量\n实践验证CBOW适用于小型数据集, 而Skip-Gram在大型语料中表现更好\n\n\n&#x3D;&#x3D;ELMo&#x3D;&#x3D;：首先在大型语料库上训练语言模型，得到词向量模型，然后在特定任务上对模型进行微调，得到更适合该任务的词向量，ELMo首次将预训练思想引入到词向量的生成中，使用双向LSTM结构，能够捕捉到词汇的上下文信息，生成更加丰富和准确的词向量表示\n\n\n\n","tags":["NLP","LLM"]}]
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="keywords" content="blog,coding,ML,DL,RL,FL">
    <meta name="description" content="learning, coding and sharing">
    <meta name="author" content="SowingG">
    
    <title>
        
            从零搭建一个Transformer |
        
        SowingG&#39;s Web
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
        <link rel="shortcut icon" href="/images/logo.png">
    
    
<link rel="stylesheet" href="/font/css/fontawesome.min.css">

    
<link rel="stylesheet" href="/font/css/regular.min.css">

    
<link rel="stylesheet" href="/font/css/solid.min.css">

    
<link rel="stylesheet" href="/font/css/brands.min.css">

    
    <script class="keep-theme-configurations">
    const KEEP = window.KEEP || {}
    KEEP.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.json"}
    KEEP.theme_config = {"base_info":{"primary_color":"#0066cc","title":"SowingG's Web","author":"SowingG","avatar":"/images/avatar.jpeg","logo":"/images/logo.png","favicon":"/images/logo.png","mode":"light"},"menu":{"home":"/","archives":"/archives","tags":"/tags","about":"/about"},"first_screen":{"enable":true,"background_img":"/images/bg.svg","background_img_dark":"/images/bg.svg","description":"Keep Sowing | Keep Growing","hitokoto":false},"social_contact":{"enable":true,"links":{"github":"https://github.com/SowingG2333","weixin":null,"qq":null,"weibo":null,"zhihu":"https://www.zhihu.com/people/samura1-43","twitter":null,"x":null,"facebook":null,"email":"donghangduan@gmail.com"}},"scroll":{"progress_bar":true,"percent":true,"hide_header":true},"home":{"announcement":null,"category":false,"tag":true,"post_datetime":"created"},"post":{"author_badge":{"enable":false,"level_badge":true,"custom_badge":["One","Two","Three"]},"word_count":{"wordcount":true,"min2read":false},"datetime_format":"YYYY-MM-DD HH:mm:ss","copyright_info":true,"share":true,"reward":{"enable":false,"img_link":null,"text":null,"icon":null},"img_align":"left"},"code_block":{"tools":{"enable":true,"style":"mac"},"highlight_theme":"obsidian"},"toc":{"enable":true,"number":false,"expand_all":true,"init_open":true,"layout":"right"},"website_count":{"busuanzi_count":{"enable":true,"site_uv":true,"site_pv":true,"page_pv":true}},"local_search":{"enable":true,"preload":false},"comment":{"enable":true,"use":"twikoo","valine":{"appid":null,"appkey":null,"server_urls":null,"placeholder":null},"twikoo":{"env_id":"https://my-vercel-sowingg2333s-projects.vercel.app","region":null,"version":"1.6.42"},"waline":{"server_url":null,"reaction":false,"version":"3.3.2"},"giscus":{"repo":null,"repo_id":null,"category":"Announcements","category_id":null,"reactions_enabled":false},"artalk":{"server":null},"disqus":{"shortname":null}},"rss":{"enable":false},"lazyload":{"enable":true},"cdn":{"enable":false,"provider":"cdnjs"},"pjax":{"enable":true},"footer":{"since":2025,"word_count":true,"site_deploy":{"enable":true,"provider":"github","url":null},"record":{"enable":false,"list":[{"code":null,"link":null}]}},"inject":{"enable":false,"css":[null],"js":[null]},"article_excerpt":{"show":false,"length":150},"root":"","source_data":{},"version":"4.2.5"}
    KEEP.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"}
    KEEP.language_code_block = {"copy":"Copy code","copied":"Copied","fold":"Fold code block","folded":"Folded"}
    KEEP.language_copy_copyright = {"copy":"Copy copyright info","copied":"Copied","title":"Original post title","author":"Original post author","link":"Original post link"}
  </script>
<meta name="generator" content="Hexo 7.3.0"></head>


<body>
<div class="progress-bar-container">
    
        <span class="scroll-progress-bar"></span>
    

    
        <span class="pjax-progress-bar"></span>
        <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i>
    
</div>



<main class="page-container border-box">
    <!-- home first screen  -->
    

    <!-- page content -->
    <div class="page-main-content border-box">
        <div class="page-main-content-top">
            $pc-search-icon-font-size = 1.5rem
$menu-bar-line-height = 2.5px
$menu-icon-gap = 0.1rem
$pc-menu-item-gap = 1.8rem
$mobile-menu-item-gap = 1.2rem
$logo-image-box-width = 2.68rem
$mobile-menu-icon-wrap-width = 1.6rem
$mobile-menu-icon-wrap-margin = 0.4rem


.header-wrapper {
  display flex
  align-items center
  justify-content center
  box-sizing border-box
  width 100%
  height 100%
  padding-top var(--header-scroll-progress-bar-height)
  background var(--background-color-1)
  box-shadow 1px 2px 6px var(--shadow-color)

  &:hover {
    box-shadow 1px 2px 10px var(--shadow-hover-color)
  }


  .header-content {
    z-index $z-index-5
    display flex
    flex-direction row
    align-items center
    justify-content space-between
    width var(--page-content-width)
    max-width var(--page-content-max-width)
    height 100%
    transition-t("max-width, width", "0, 0", "0.1, 0.1", "ease, ease")

    &.has-first-screen {
      max-width var(--page-content-max-width-2)
    }


    .has-toc & {
      max-width var(--page-content-max-width-2)
    }


    +keep-tablet() {
      width var(--page-content-width-tablet)
    }


    +keep-mobile() {
      width var(--page-content-width-mobile)
    }


    .left {
      height 100%
      font-size 2rem

      transition-t("transform, transform-origin", "0, 0", "0.2, 0.2", "linear, linear")

      .header-shrink & {
        transform scale(0.78)
        transform-origin left
      }


      if (hexo-config('base_info') && hexo-config('base_info.logo')) {
        .logo-image {
          flex-shrink 0
          width $logo-image-box-width
          height $logo-image-box-width
          margin-right 0.5rem


          +keep-tablet() {
            width $logo-image-box-width * 0.9
            height $logo-image-box-width * 0.9
          }


          +keep-mobile() {
            width $logo-image-box-width * 0.8
            height $logo-image-box-width * 0.8
          }


          img {
            width 100%
            border-radius 0.4rem
          }
        }
      }


      .site-name {
        color var(--text-color-1)
        font-weight 600
        font-size var(--header-title-font-size)
        font-family var(--header-title-font-family)
        line-height 1
        letter-spacing 1px

        +keep-tablet() {
          font-size calc(var(--header-title-font-size) * 0.9)
        }


        +keep-mobile() {
          font-size calc(var(--header-title-font-size) * 0.8)
        }


        .is-home & {
          if (hexo-config('first_screen') && hexo-config('first_screen.enable') == true) {
            color var(--first-screen-header-font-color-light)
            filter brightness(92%)

            .dark-mode & {
              color var(--first-screen-header-font-color-dark)
              filter brightness(106%)
            }
          }
        }


        .show-header-drawer & {
          color var(--text-color-1) !important
        }
      }
    }


    .right {
      .pc {
        .menu-list {
          display flex
          gap $pc-menu-item-gap
          align-items center

          +keep-tablet() {
            display none
          }


          .menu-item {
            font-size 1rem
            cursor pointer

            &.search {
              font-size $pc-search-icon-font-size
              &::before {
                display none !important
              }
            }


            .menu-text-color {
              color var(--text-color-3)
            }


            &::before {
              position absolute
              bottom -10px
              left 50%
              box-sizing border-box
              width 0
              height 2.6px
              background var(--primary-color)
              border-radius 3px
              transform translateX(-50%)
              content ''
              transition-t("transform, bottom, width", "0, 0, 0", "0.2, 0.2, 0.2", "linear, linear, ease")

              .header-shrink & {
                bottom calc(-1 * calc(var(--header-shrink-height) * 0.5 - 12px))
              }
            }


            &:hover
            &.active {
              &::before {
                width 100%
              }
            }


            .menu-icon {
              display var(--header-menu-icon)
              margin-right $menu-icon-gap
            }


            .is-home & {
              if (hexo-config('first_screen') && hexo-config('first_screen.enable') == true) {
                .menu-text-color {
                  color var(--first-screen-header-font-color-light)
                }


                .dark-mode & {
                  .menu-text-color {
                    color var(--first-screen-header-font-color-dark)
                  }
                }
              }
            }


            &.has-sub-menu {
              &::after {
                position absolute
                bottom 0
                left 0
                box-sizing border-box
                width 100%
                height 100%
                content ''
              }

              &:hover {
                &::after {
                  bottom -100%
                }

                .collapse-icon {
                  transform rotate(180deg)
                }

                .sub-menu-list {
                  display flex
                }
              }
            }


            .collapse-icon {
              margin-left $menu-icon-gap
              transition-t("transform", "0", "0.3", "ease")
            }
          }


          .sub-menu-list {
            position absolute
            bottom -280%
            left 50%
            display none
            gap $pc-menu-item-gap
            justify-content flex-start
            box-sizing border-box
            padding 0.8rem 1.6rem
            background var(--background-color-1)
            border-radius 0.6rem
            box-shadow 0 0 8px var(--shadow-color)
            transform translateX(-50%)

            .sub-menu-item {
              white-space nowrap

              .menu-text-color {
                color var(--text-color-3) !important
              }

              &:hover
              &.active {
                .menu-text-color {
                  color var(--primary-color) !important
                }
              }

              .sub-menu-icon {
                display var(--header-menu-icon)
                margin-right $menu-icon-gap * 4
              }
            }
          }
        }
      }


      .mobile {
        gap 1rem

        .icon-item {
          position relative
          display none
          width 20px
          height 20px
          color var(--text-color-3)
          font-size 1.2rem
          cursor pointer

          i {
            color var(--text-color-3)

            .show-header-drawer & {
              color var(--text-color-3) !important
            }

            .reset-color & {
              color var(--text-color-3) !important
            }
          }


          +keep-tablet() {
            display flex
            align-items center
            justify-content center
          }
        }


        .menu-bar {
          .menu-bar-middle {
            position relative
            width 18px
            height $menu-bar-line-height
            background var(--text-color-3)

            &::before
            &::after {
              position absolute
              left 0
              width 100%
              height $menu-bar-line-height
              background var(--text-color-3)
              content ''
              transition-t("transform", "0", "0.38", "ease")
            }


            &::before {
              top -6px

              .show-header-drawer & {
                transform translateY(6px) rotate(45deg)
              }
            }


            &::after {
              bottom -6px

              .show-header-drawer & {
                transform translateY(-6px) rotate(-45deg)
              }
            }


            .reset-color & {
              background var(--text-color-3) !important

              &::before
              &::after {
                background var(--text-color-3) !important
              }
            }


            .show-header-drawer & {
              background transparent !important

              &::before
              &::after {
                background var(--text-color-3) !important
              }
            }
          }
        }


        .is-home & {
          if (hexo-config('first_screen') && hexo-config('first_screen.enable') == true) {
            .icon-item i {
              color var(--first-screen-header-font-color-light)
            }

            .menu-bar .menu-bar-middle {
              background var(--first-screen-header-font-color-light)

              &::before
              &::after {
                background var(--first-screen-header-font-color-light)
              }
            }

            .dark-mode & {
              .icon-item i {
                color var(--first-screen-header-font-color-dark)
              }

              .menu-bar .menu-bar-middle {
                background var(--first-screen-header-font-color-dark)

                &::before
                &::after {
                  background var(--first-screen-header-font-color-dark)
                }
              }
            }
          }
        }
      }
    }
  }


  .header-drawer {
    position absolute
    top 0
    left 0
    z-index $z-index-2
    display none
    justify-content center
    box-sizing border-box
    width 100%
    padding var(--header-height) 0 2rem 0
    background var(--background-color-1)
    transform scaleY(0)
    transform-origin top
    transition-t("transform", "0", "0.38", "ease")

    +keep-tablet() {
      display flex
    }


    .show-header-drawer & {
      transform scaleY(1)
    }


    .drawer-menu-list {
      display flex
      flex-direction column
      gap 1.2rem
      align-items flex-start
      justify-content flex-start
      width var(--page-content-width)
      max-width var(--page-content-max-width)
      height 100%
      margin-top 1rem

      +keep-tablet() {
        width var(--page-content-width-tablet)
      }


      +keep-mobile() {
        width var(--page-content-width-mobile)
      }


      .drawer-menu-item {
        width 100%
        padding 0.6rem 0
        font-size 1.06rem
        border-bottom 0.1rem solid var(--background-color-3)

        &.active {
          .drawer-menu-label {
            .drawer-menu-text-color {
              color var(--primary-color)
            }
          }
        }


        &.show-sub-menu {
          .collapse-icon {
            transform rotate(-90deg)
          }


          .drawer-sub-menu-list {
            display flex
            height auto
          }
        }


        .drawer-menu-label {
          display flex
          align-items center
          justify-content space-between
          width 100%

          .left-side {
            width 100%

            .menu-icon-wrap {
              display var(--header-menu-icon)
              width $mobile-menu-icon-wrap-width
              margin-right $mobile-menu-icon-wrap-margin
            }
          }


          .right-side {
            margin-left 0.4rem
            cursor pointer
            transition-t("transform", "0", "0.2", "ease")
          }
        }
      }


      .drawer-sub-menu-list {
        position relative
        display none
        flex-direction column
        justify-content flex-start
        box-sizing border-box
        width 100%
        height 0
        padding-top 0.6rem
        padding-bottom 0.6rem
        padding-left $mobile-menu-icon-wrap-width
        transition-t("display, height", "0, 0", "0.2, 0.2", "linear, linear")
        disable-user-select()

        .sub-menu-item {
          margin-top 1rem
          padding 0.6rem 0

          &.active {
            a
            a i {
              color var(--primary-color)
            }
          }


          .sub-menu-icon-wrap {
            display var(--header-menu-icon)
            width $mobile-menu-icon-wrap-width
            margin-right $mobile-menu-icon-wrap-margin
          }
        }
      }
    }
  }


  .window-mask {
    position absolute
    top 0
    left 0
    z-index $z-index-1
    display none
    box-sizing border-box
    width 100%
    height 100vh
    background rgba(0, 0, 0, 0.4)
    visibility hidden
    opacity 0
    transition-t("transform, opacity", "0, 0", "0.38, 0.38", "ease, ease")

    +keep-tablet() {
      display block
    }


    .show-header-drawer & {
      visibility visible
      opacity 1
    }
  }
}


.show-header-drawer {
  overflow hidden
}

        </div>

        <div class="page-main-content-middle border-box">

            <div class="main-content border-box">
                

                    
<div class="fade-in-down-animation">
    <div class="post-page-container border-box">
        <div class="post-content-container border-box">
            

            <div class="post-content-bottom border-box">
                
                    <div class="post-title">
                        从零搭建一个Transformer
                    </div>
                

                
                    <div class="post-header border-box">
                        
                            <div class="avatar-box border-box">
                                <img src="/images/avatar.jpeg">
                            </div>
                        
                        <div class="info-box">
                            <div class="author border-box">
                                <span class="name">SowingG</span>
                                
                            </div>
                            <div class="meta-info border-box">
                                

<div class="post-meta-info-container border-box post">
    <div class="post-meta-info border-box">
        

        
            <span class="meta-info-item post-create-date">
                <i class="icon fa-solid fa-calendar-plus"></i>&nbsp;
                <span class="datetime">2025-09-08 18:00:00</span>
            </span>

            
        

        

        
            <span class="post-tag meta-info-item border-box">
                <ul class="post-tag-ul">
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/NLP/">NLP</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/LLM/">LLM</a></li>
                        
                    
                            <li class="tag-item"><span class="tag-separator"><i class="icon fas fa-hashtag"></i></span><a href="/tags/Transformer/">Transformer</a></li>
                        
                    
                </ul>
            </span>
        

        
        
            <span class="meta-info-item post-wordcount">
                <i class="icon fas fa-file-word"></i>&nbsp;<span>2.6k Words</span>
            </span>
        
        
        
            <span class="meta-info-item post-pv">
                <i class="icon fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
            </span>
        
    </div>

    
</div>

                            </div>
                        </div>
                    </div>
                

                <div class="post-content keep-markdown-body ">
                    

                    
                         <h1 id="从零搭建一个transformer">从零搭建一个Transformer</h1>
<h2 id="前置知识">前置知识</h2>
<ul>
<li>深度学习基础</li>
<li>线性代数</li>
<li>概率论与数理统计</li>
</ul>
<h2 id="i.-从零运行一个transformer">I. 从零运行一个Transformer</h2>
<h3 id="encoder部分">Encoder部分</h3>
<p>假设我们输入transformer的是<code>“我爱学习”</code>这一句话。</p>
<p>这句话首先会经过一个tokenizer（分词器），变成<code>“我，爱，学习”</code>这三个词语，也就是我们所说的token。</p>
<p>我们会构建一个词表，将每个token映射到对应的id，我们这里假设词表仅有上文三个词：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;“我”: <span class="number">1</span>, “爱”:<span class="number">2</span>, “学习”: <span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure>
<p>这样，我们的输入就可以表征为一个序列长度<code>seq_l</code>为3一维张量
<code>[1, 2, 3]</code> 。</p>
<p>虽然我们能够把自然语言通过这种方式数学化地表示出来，但是却损失了本来语句中包含的信息（1,
2,
3这三个数字之间并没有包含任何语义和语法上的信息和关联），所以我们需要用更复杂的方式来表征我们的token，而这种方式就是更长的向量。</p>
<p>我们如果能够让语义相近的词在向量空间中距离更近（“番茄”和“西红柿”），以及多个语义相叠加或排除能够得到一个新的语义（女人+警察=女警
/
警察-男人=女警），就能够通过向量很好地表征自然语言的信息，而从token到语义向量的这个过程我们可以通过机器学习来完成。</p>
<p>我们先将token转化为独热编码的形式，以便于进行矩阵运算，这个时候我们的输入就变成了这样一个<code>[3, 3]</code>的矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>] [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]]</span><br></pre></td></tr></table></figure>
<p>我们用X来表示当前的输入，X会与一个名为嵌入矩阵（embedding）的矩阵<span
class="math inline"><em>W</em><sub><em>e</em><em>m</em><em>b</em><em>e</em><em>d</em></sub></span>相乘，这个矩阵通过反向传播来调整参数，从而使得X能够变成包含语义信息的矩阵。</p>
<p>嵌入矩阵的大小为<code>[vocab_size, d_m]</code>，
其中<code>vocab_size</code>是指词表大小（在我们的例子中为3），而<code>d_m</code>则是后续的模型参数大小，我们假设模型参数大小为9，则此时的嵌入矩阵大小为<code>[3, 9]</code></p>
<p>经过下列运算我们得到：</p>
<p><span
class="math inline"><em>X</em><em>W</em><sub><em>e</em><em>m</em><em>b</em><em>e</em><em>d</em></sub> = <em>H</em></span></p>
<p>此时的<span
class="math inline"><em>H</em></span>正是我们真正输入transformer的矩阵，其大小为<code>[seq_l, d_m]</code>
：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.1</span>, <span class="number">0.5</span>, <span class="number">0.2</span>], [<span class="number">0.3</span>, -<span class="number">0.4</span>, <span class="number">0.1</span>], [<span class="number">0.3</span>, <span class="number">0.2</span>, <span class="number">0.9</span>]]</span><br></pre></td></tr></table></figure>
<p>由于transformer的注意力机制无法像RNN那样自然地关注到序列的位置信息，所以这里需要对每个token向量加上一个positional
code，具体公式如下：</p>
<p><span class="math inline">$PE_{(pos, 2i)} =
\sin(\frac{pos}{10000^{2i/d_m}})$</span></p>
<p><span class="math inline">$PE_{(pos, 2i+1)} =
\cos(\frac{pos}{10000^{2i/d_m}})$</span></p>
<p>所以此时的输入<span class="math inline"><em>H</em></span>变成了：</p>
<p><span
class="math inline"><em>H</em> + <em>P</em><em>E</em> = <em>H</em><sup>′</sup></span></p>
<p>其中PE是位置编码矩阵，其大小与H相同，也为<code>[seq_l, d_m]</code>
。这样，每个token的向量表示中就包含了它在序列中的位置信息。接下来，我们需要将这个输入H′送入Transformer的核心组件——多头自注意力机制（Multi-head
Self-Attention）。</p>
<p>Transformer的多头自注意力机制通过<code>Q, K, V</code>三个核心矩阵来完成（Query,
Key和Value）</p>
<p>在多头自注意力机制中，我们首先计算Q, K, V矩阵：</p>
<p><span
class="math inline"><em>Q</em> = <em>H</em><sup>′</sup><em>W</em><sub><em>Q</em></sub></span></p>
<p><span
class="math inline"><em>K</em> = <em>H</em><sup>′</sup><em>W</em><sub><em>K</em></sub></span></p>
<p><span
class="math inline"><em>V</em> = <em>H</em><sup>′</sup><em>W</em><sub><em>V</em></sub></span></p>
<p>其中，<span
class="math inline"><em>W</em><sub><em>Q</em></sub>, <em>W</em><sub><em>K</em></sub>, <em>W</em><sub><em>V</em></sub></span>是参数矩阵，它们的大小都是<code>[d_m, d_k]</code>，其中<code>d_k = d_m / h</code>，<code>h</code>是注意力头的数量。</p>
<p>我们假设有3个注意力头，那么就会存在三组参数矩阵，生成三组<code>Q, K, V</code>
。接下来，对于每个注意力头，我们计算注意力得分矩阵：</p>
<p><span class="math inline">$Attention(Q, K, V) =
softmax(\frac{QK^T}{\sqrt{d_k}})V$</span></p>
<p>在我们的例子中：<code>d_k=3</code> ，而<span
class="math inline">$\frac{1}{\sqrt{d_k}}$</span>是一个缩放因子，用于防止softmax数值爆炸的问题。</p>
<p>在注意力公式中，<span
class="math inline"><em>Q</em><em>K</em><sup><em>T</em></sup></span>可以视作Q和K两个矩阵中的向量进行点积；对于第一个注意力头，此时的结果可能为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0.2</span>],</span><br><span class="line"> [<span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0.1</span>],</span><br><span class="line"> [<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.6</span>]]</span><br></pre></td></tr></table></figure>
<p>这个矩阵表示每个token对其他token的关注程度。</p>
<p>例如，第一行表示”我”这个token对”我”、“爱”和”学习”这三个token的关注度分别为0.7、0.1和0.2。</p>
<p>经过softmax归一化后，这些注意力权重将用于对V矩阵进行加权求和，从而生成新的上下文表示。</p>
<p>计算出每个注意力头的注意力得分矩阵后，我们将这些结果拼接起来，并通过一个线性变换，得到多头注意力的输出：</p>
<p><span
class="math inline"><em>M</em><em>u</em><em>l</em><em>t</em><em>i</em><em>H</em><em>e</em><em>a</em><em>d</em>(<em>Q</em>, <em>K</em>, <em>V</em>) = <em>C</em><em>o</em><em>n</em><em>c</em><em>a</em><em>t</em>(<em>h</em><em>e</em><em>a</em><em>d</em><sub>1</sub>, <em>h</em><em>e</em><em>a</em><em>d</em><sub>2</sub>, ..., <em>h</em><em>e</em><em>a</em><em>d</em><sub><em>h</em></sub>)<em>W</em><sub><em>O</em></sub></span></p>
<p>其中，<span
class="math inline"><em>W</em><sub><em>O</em></sub></span>是一个参数矩阵，大小为<code>[d_m, d_m]</code>。这个多头注意力的输出将进入Transformer的下一个组件——前馈神经网络（Feed-Forward
Network）。</p>
<p>对于FNN，可以理解为一个MLP，而我们最终会得到一个形状为<code>[seq_l, d_m]</code>的输出。</p>
<p>✅ 至此，transformer的encoder部分已经运行结束</p>
<h3 id="decoder部分">Decoder部分</h3>
<p>Decoder部分的运行过程类似于Encoder，但有几个关键区别：</p>
<ul>
<li>首先，Decoder是自回归（Autoregressive）的，意味着它一次只能生成一个token。</li>
<li>其次，为了防止模型在训练时”作弊”看到未来的token，我们使用掩码机制（Masked
Attention）。在实践中，这意味着我们在计算注意力分数时，将未来位置的分数设为负无穷（-inf），这样softmax后的权重就为0，模型就无法使用未来的信息。假设我们的目标输出序列是”我喜欢学习”，在生成”喜欢”这个token时，模型只能看到”我”这个已经生成的token。</li>
<li>除了掩码自注意力外，Decoder还包含一个额外的<code>编码器-解码器注意力层</code>，用于关注Encoder的输出。这使得Decoder能够在生成每个token时，考虑输入序列的全部信息。这种结构特别适合机器翻译等任务，使模型能够准确捕捉源语言和目标语言之间的对应关系。</li>
</ul>
<p>假设我们运行的transformer执行的是汉译英的翻译任务，并且已经翻译出<code>"I like"</code>（目标为<code>"I like study"</code>）：</p>
<p>我们首先将已经生成的结果通过<code>嵌入层和位置编码</code>处理，得到初始的向量表示；然后这个向量会通过<code>带掩码的自注意力层</code>，确保模型只能看到已经生成的token。</p>
<p>如果此时是推理阶段，那么掩码与否无关紧要；如果此时是训练阶段，我们要模型在生成<code>"I like"</code>
的前提下生成<code>"study"</code>
，那么在进行并行的训练时候，需要将<code>"I like study"</code>
进行掩码处理，结果形如：</p>
<p><code>&lt;BOS&gt; -inf -inf -inf -inf</code></p>
<p><code>&lt;BOS&gt; I -inf -inf -inf</code></p>
<p><code>&lt;BOS&gt; I like -inf -inf</code></p>
<p><code>&lt;BOS&gt; I like study -inf</code></p>
<p><code>&lt;BOS&gt; I like study &lt;EOS&gt;</code></p>
<p>接着通过<code>编码器-解码器注意力层</code>，模型能够关注编码器输出的信息（我们的<code>"我爱学习"</code>），从而生成相应的英文翻译<code>"study"</code>。</p>
<p>✅ 至此，transformer已完整运行一次</p>
<h2 id="qa">Q&amp;A</h2>
<h3
id="q1-归一化normalization的核心思想是什么为什么它能解决梯度问题"><strong>Q1:
归一化（Normalization）的核心思想是什么？为什么它能解决梯度问题？</strong></h3>
<ul>
<li>核心思想是让神经网络每一层的输入分布保持一致，从而解决因参数更新导致的“内部协变量偏移”（Internal
Covariate
Shift）。这使得网络学习更稳定，加速收敛，并能缓解梯度消失/爆炸问题。这里的“分布”通常指神经元激活值的均值和方差。</li>
</ul>
<h3 id="q2-为什么批量归一化bn不适用于循环神经网络rnn"><strong>Q2:
为什么批量归一化（BN）不适用于循环神经网络（RNN）？</strong></h3>
<ul>
<li><strong>分布差异</strong>：BN对一个批次中<strong>同一时间步</strong>的激活值进行归一化。但在变长序列中，这些激活值可能对应完全不同的词语（如“你”和“机器学习”），它们来自不同分布，强制归一化会破坏特征。</li>
<li><strong>变长序列问题</strong>：BN需要为每个时间步保存统计量。但测试集可能出现比训练集更长的序列，导致无法获取后续时间步的统计量。</li>
<li><strong>计算开销</strong>：BN需要为每个时间步单独计算和保存统计量，这在时间维度上增加了巨大的计算和内存开销。</li>
</ul>
<h3 id="q3-批量归一化与大数定理和中心极限定理有什么关系"><strong>Q3:
批量归一化与大数定理和中心极限定理有什么关系？</strong></h3>
<ul>
<li><strong>大数定理</strong>：BN利用大数定理的思想，用<strong>mini-batch的样本统计量</strong>来近似<strong>整个数据集的总体统计量</strong>，从而使归一化操作在每次迭代中都具有代表性。</li>
<li><strong>中心极限定理</strong>：BN利用该定理所揭示的“样本均值的分布趋近于正态分布”的规律，来<strong>保证用于标准化的统计量是稳定可靠的</strong>。这使得BN能够放心地将数据强制归一化到<strong>均值为0、方差为1</strong>的稳定分布，这才是其真正的目的，而非让数据本身变为正态分布。</li>
</ul>
<h3
id="q4-层归一化ln和批量归一化bn在参数更新和效率上有何不同"><strong>Q4:
层归一化（LN）和批量归一化（BN）在参数更新和效率上有何不同？</strong></h3>
<ul>
<li><strong>参数更新频率</strong>：无论是BN还是LN，它们的可学习参数（γ和<span
class="math inline"><em>β</em></span>）以及模型的其他参数，都是<strong>每个训练批次（batch）更新一次</strong>。</li>
<li><strong>效率</strong>：LN的效率通常更高。因为LN对每个样本独立计算统计量，其计算模式非常适合现代GPU的<strong>并行计算</strong>。而BN需要先汇总整个批次的统计量，会产生额外的同步开销。</li>
<li><strong>推理</strong>：LN的归一化只依赖于单个样本，因此训练和测试时的行为完全一致，<strong>无需保存任何全局统计量</strong>。而BN必须在训练时保存一个全局统计量的运行平均值，用于测试时使用。</li>
</ul>
<h3 id="q5-transformer模型如何处理长序列"><strong>Q5:
Transformer模型如何处理长序列？</strong></h3>
<ul>
<li>Transformer不使用循环，而是通过<strong>自注意力机制</strong>进行<strong>并行计算</strong>。它使用<strong>位置编码</strong>来提供序列中的位置信息，并用掩码（Masking）来处理变长序列中的填充部分，确保模型不会将注意力放在无意义的填充词上。</li>
</ul>
<h3
id="q6-在大模型中嵌入层embedding-layer的参数量是如何计算的"><strong>Q6:
在大模型中，嵌入层（Embedding
Layer）的参数量是如何计算的？</strong></h3>
<ul>
<li>嵌入层的参数量非常大，它等于<strong>词汇量大小（vocab_size）乘以嵌入维度（embedding_dim或d_model）</strong>。例如，一个有5万词汇，嵌入维度为1024的模型，其嵌入层参数量就是50,000
* 1024。</li>
</ul>

                    
                </div>

                
                        
<div class="post-copyright-info-container border-box">
    <div class="copyright-info-content border-box">
        <div class="copyright-info-top border-box">
            <div class="copyright-post-title border-box text-ellipsis">
                从零搭建一个Transformer
            </div>

            <div class="copyright-post-link border-box text-ellipsis">
                2025/09/08/从零搭建一个Transformer/
            </div>
        </div>

        <div class="copyright-info-bottom border-box">
            <div class="copyright-post-author bottom-item">
                <div class="type">
                    Author
                </div>
                <div class="content">SowingG</div>
            </div>

            <div class="post-time bottom-item">
                <div class="type">
                    Published
                </div>
                <div class="content">2025-09-08 18:00</div>
            </div>


            <div class="post-license bottom-item">
                <div class="type">
                    License
                </div>
                <div class="content tooltip" data-tooltip-content="CC BY-NC-SA 4.0">
                    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed" target="_blank">
                        
                            <i class="fa-brands fa-creative-commons"></i>
                            <i class="fa-brands fa-creative-commons-by"></i>
                            <i class="fa-brands fa-creative-commons-nc"></i>
                            <i class="fa-brands fa-creative-commons-sa"></i>
                        
                    </a>
                </div>
            </div>
        </div>

        <i class="copyright-bg fa-solid fa-copyright"></i>
    </div>
    <div class="copy-copyright-info flex-center tooltip" data-tooltip-content="Copy copyright info" data-tooltip-offset-y="-2px">
        <i class="fa-solid fa-copy"></i>
    </div>
</div>

                

                <div class="post-bottom-tags-and-share border-box">
                    <div>
                        
                            <ul class="post-tags-box border-box">
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/NLP/">NLP</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/LLM/">LLM</a>
                                    </li>
                                
                                    <li class="tag-item border-box">
                                        <i class="icon fas fa-hashtag"></i>&nbsp;<a href="/tags/Transformer/">Transformer</a>
                                    </li>
                                
                            </ul>
                        
                    </div>
                    <div>
                        
                            <div class="post-share-container border-box">
    <ul class="share-list-wrap border-box">
        <li class="qq share-item border-box flex-center tooltip"
            data-tooltip-content="Share to QQ"
        >
            <i class="fa-brands fa-qq"></i>
        </li>
        <li class="wechat share-item border-box flex-center tooltip tooltip-img"
            data-tooltip-content="Share to WeChat"
            data-tooltip-img-tip="Scan by WeChat"
            data-tooltip-img-style="background-color: #fff; top: -10px; padding: 0.6rem 0.6rem 0.1rem 0.6rem;"
        >
            <i class="fa-brands fa-weixin"></i>
        </li>
        <li class="weibo share-item border-box flex-center tooltip"
            data-tooltip-content="Share to WeiBo"
        >
            <i class="fa-brands fa-weibo"></i>
        </li>
    </ul>
</div>

                        
                    </div>
                </div>

                

                
                    <div class="post-nav border-box">
                        
                        
                            <div class="next-post">
                                <a class="next"
                                   rel="next"
                                   href="/2025/09/07/LLM-Intro/"
                                   title="LLM Intro"
                                >
                                    <span class="title flex-center">
                                        <span class="post-nav-title-item text-ellipsis">LLM Intro</span>
                                        <span class="post-nav-item">Next posts</span>
                                    </span>
                                    <span class="right arrow-icon flex-center">
                                        <i class="fas fa-chevron-right"></i>
                                    </span>
                                </a>
                            </div>
                        
                    </div>
                

                
                    


    <div class="comments-container border-box">
        <div id="comments-anchor" class="comment-area-title border-box">
            <i class="fas fa-comments"></i>&nbsp;Comments
        </div>
        <div class="comment-plugin-fail border-box">
    <span class="fail-tip">Comment plugin failed to load</span>
    <button class="reload keep-button">Click to reload</button>
</div>
<div class="comment-plugin-loading flex-center border-box">
    <i class="loading-icon fa-solid fa-spinner fa-spin"></i>
    <span class="load-tip">Loading comment plugin</span>
</div>
<script data-pjax>
  window.KeepCommentPlugin = {}
  window.KeepCommentPlugin.hideLoading = () => {
    const cplDom = document.querySelector('.comments-container .comment-plugin-loading')
    cplDom.style.display = 'none'
  }
  window.KeepCommentPlugin.loadFailHandle = () => {
    window.KeepCommentPlugin.hideLoading()
    const cpfDom = document.querySelector('.comments-container .comment-plugin-fail')
    cpfDom.style.display = 'flex'
    cpfDom.querySelector('.reload').addEventListener('click', () => {
      window.location.reload()
    })
  }
</script>

        
            

    <div class="twikoo-container">
        <div id="twikoo-comment"></div>
        <script data-pjax
                src="//cdn.jsdelivr.net/npm/twikoo@1.6.42/dist/twikoo.all.min.js"
                async
                onerror="window.KeepCommentPlugin.loadFailHandle()"
        ></script>
        <script data-pjax
                async
                onerror="window.KeepCommentPlugin.loadFailHandle()"
        >
          window.KeepCommentPlugin.initTwikoo = () => {
            if (window?.twikoo) {
              twikoo.init({
                el: '#twikoo-comment',
                envId: 'https://my-vercel-sowingg2333s-projects.vercel.app',
                region: '',
                lang: 'en' || 'zh-CN'
              })
              window.KeepCommentPlugin.hideLoading()
            } else {
              setTimeout(() => {
                window.KeepCommentPlugin.initTwikoo()
              }, 1000)
            }
          }

          if ('true' === 'true') {
            setTimeout(() => {
              window.KeepCommentPlugin.initTwikoo()
            }, 1200)
          } else {
            window.addEventListener('DOMContentLoaded', window.KeepCommentPlugin.initTwikoo)
          }
        </script>
    </div>


        
    </div>





                
            </div>
        </div>

        
            <div class="pc-post-toc right-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AAtransformer"><span class="nav-text">从零搭建一个Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-text">前置知识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#i.-%E4%BB%8E%E9%9B%B6%E8%BF%90%E8%A1%8C%E4%B8%80%E4%B8%AAtransformer"><span class="nav-text">I. 从零运行一个Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder%E9%83%A8%E5%88%86"><span class="nav-text">Encoder部分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder%E9%83%A8%E5%88%86"><span class="nav-text">Decoder部分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qa"><span class="nav-text">Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-%E5%BD%92%E4%B8%80%E5%8C%96normalization%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E6%98%AF%E4%BB%80%E4%B9%88%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%83%E8%83%BD%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E9%97%AE%E9%A2%98"><span class="nav-text">Q1:
归一化（Normalization）的核心思想是什么？为什么它能解决梯度问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96bn%E4%B8%8D%E9%80%82%E7%94%A8%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn"><span class="nav-text">Q2:
为什么批量归一化（BN）不适用于循环神经网络（RNN）？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E4%B8%8E%E5%A4%A7%E6%95%B0%E5%AE%9A%E7%90%86%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86%E6%9C%89%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB"><span class="nav-text">Q3:
批量归一化与大数定理和中心极限定理有什么关系？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q4-%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96ln%E5%92%8C%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96bn%E5%9C%A8%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E5%92%8C%E6%95%88%E7%8E%87%E4%B8%8A%E6%9C%89%E4%BD%95%E4%B8%8D%E5%90%8C"><span class="nav-text">Q4:
层归一化（LN）和批量归一化（BN）在参数更新和效率上有何不同？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q5-transformer%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E9%95%BF%E5%BA%8F%E5%88%97"><span class="nav-text">Q5:
Transformer模型如何处理长序列？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q6-%E5%9C%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E5%B5%8C%E5%85%A5%E5%B1%82embedding-layer%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%9A%84"><span class="nav-text">Q6:
在大模型中，嵌入层（Embedding
Layer）的参数量是如何计算的？</span></a></li></ol></li></ol></li></ol>
    </div>
</div>

            </div>
        
    </div>
</div>


                
            </div>
        </div>

        <div class="page-main-content-bottom border-box">
            
<footer class="footer border-box">
    <div class="copyright-info info-item">
    &copy;&nbsp;2025
    
            &nbsp;<i class="fas fa-heart icon-animate"></i>&nbsp;&nbsp;<a href="/">SowingG</a>
        
    </div>

    <div class="theme-info info-item">
        Powered by&nbsp;<a target="_blank" href="https://hexo.io">Hexo</a>&nbsp;&&nbsp;Theme&nbsp;<a class="keep-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep</a>
    </div>

    
        
        <div class="deploy-info info-item">
            
            This site is deployed on <span class="tooltip" data-tooltip-content="GitHub Pages"><img src="/images/brands/github.png"></span>
            
        </div>
    

    
        <div class="count-info info-item">
            
                <span class="count-item border-box word">
                    <span class="item-type border-box">Total words</span>
                    <span class="item-value border-box word">3.2k</span>
                </span>
            

            
                <span class="count-item border-box uv">
                    <span class="item-type border-box">Unique Visitor</span>
                    <span class="item-value border-box uv" id="busuanzi_value_site_uv"></span>
                </span>
            

            
                <span class="count-item border-box pv">
                    <span class="item-type border-box">Page View</span>
                    <span class="item-value border-box pv" id="busuanzi_value_site_pv"></span>
                </span>
            
        </div>
    

    
</footer>

        </div>
    </div>

    <!-- post tools -->
    
        <div class="post-tools right-toc">
            <div class="post-tools-container border-box">
    <ul class="post-tools-list border-box">
        <!-- PC encrypt again -->
        

        <!-- PC TOC show toggle -->
        
            <li class="tools-item flex-center toggle-show-toc">
                <i class="fas fa-list"></i>
            </li>
        

        <!-- PC go comment -->
        
            <li class="tools-item flex-center go-to-comments">
                <i class="fas fa-comment"></i>
                <span class="post-comments-count"></span>
            </li>
        

        <!-- PC full screen -->
        <li class="tools-item flex-center full-screen">
            <i class="fa-solid fa-expand"></i>
        </li>
    </ul>
</div>

        </div>
    

    <!-- side tools -->
    <div class="side-tools">
        <div class="side-tools-container border-box ">
    <ul class="side-tools-list side-tools-show-handle border-box">
        <li class="tools-item tool-font-adjust-plus flex-center">
            <i class="fas fa-search-plus"></i>
        </li>

        <li class="tools-item tool-font-adjust-minus flex-center">
            <i class="fas fa-search-minus"></i>
        </li>

        <!-- toggle mode -->
        

        <!-- rss -->
        

        <!-- to bottom -->
        <li class="tools-item tool-scroll-to-bottom flex-center">
            <i class="fas fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="exposed-tools-list border-box">
        
            <li class="tools-item toggle-show-toc-tablet flex-center">
                <i class="fas fa-list"></i>
            </li>
        

        
            <li class="tools-item go-to-comments-tablet flex-center">
                <i class="fas fa-comment"></i>
            </li>
        

        <li class="tools-item tool-toggle-show flex-center">
            <i class="fas fa-cog fa-spin"></i>
        </li>

        <li class="tools-item tool-scroll-to-top flex-center show-arrow">
            <i class="arrow fas fa-arrow-up"></i>
            <span class="percent"></span>
        </li>
    </ul>
</div>

    </div>

    <!-- image mask -->
    <div class="zoom-in-image-mask">
    <img class="zoom-in-image">
</div>


    <!-- local search -->
    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fas fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="Search..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="close-popup-btn">
                <i class="fas fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    

    <!-- tablet toc -->
    
        <div class="tablet-post-toc-mask">
            <div class="tablet-post-toc">
                <div class="post-toc-wrap border-box">
    <div class="post-toc border-box">
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AAtransformer"><span class="nav-text">从零搭建一个Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="nav-text">前置知识</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#i.-%E4%BB%8E%E9%9B%B6%E8%BF%90%E8%A1%8C%E4%B8%80%E4%B8%AAtransformer"><span class="nav-text">I. 从零运行一个Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#encoder%E9%83%A8%E5%88%86"><span class="nav-text">Encoder部分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#decoder%E9%83%A8%E5%88%86"><span class="nav-text">Decoder部分</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#qa"><span class="nav-text">Q&amp;A</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#q1-%E5%BD%92%E4%B8%80%E5%8C%96normalization%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E6%98%AF%E4%BB%80%E4%B9%88%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AE%83%E8%83%BD%E8%A7%A3%E5%86%B3%E6%A2%AF%E5%BA%A6%E9%97%AE%E9%A2%98"><span class="nav-text">Q1:
归一化（Normalization）的核心思想是什么？为什么它能解决梯度问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q2-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96bn%E4%B8%8D%E9%80%82%E7%94%A8%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn"><span class="nav-text">Q2:
为什么批量归一化（BN）不适用于循环神经网络（RNN）？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q3-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96%E4%B8%8E%E5%A4%A7%E6%95%B0%E5%AE%9A%E7%90%86%E5%92%8C%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86%E6%9C%89%E4%BB%80%E4%B9%88%E5%85%B3%E7%B3%BB"><span class="nav-text">Q3:
批量归一化与大数定理和中心极限定理有什么关系？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q4-%E5%B1%82%E5%BD%92%E4%B8%80%E5%8C%96ln%E5%92%8C%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96bn%E5%9C%A8%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E5%92%8C%E6%95%88%E7%8E%87%E4%B8%8A%E6%9C%89%E4%BD%95%E4%B8%8D%E5%90%8C"><span class="nav-text">Q4:
层归一化（LN）和批量归一化（BN）在参数更新和效率上有何不同？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q5-transformer%E6%A8%A1%E5%9E%8B%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E9%95%BF%E5%BA%8F%E5%88%97"><span class="nav-text">Q5:
Transformer模型如何处理长序列？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#q6-%E5%9C%A8%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%AD%E5%B5%8C%E5%85%A5%E5%B1%82embedding-layer%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F%E6%98%AF%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%9A%84"><span class="nav-text">Q6:
在大模型中，嵌入层（Embedding
Layer）的参数量是如何计算的？</span></a></li></ol></li></ol></li></ol>
    </div>
</div>

            </div>
        </div>
    
</main>





<!-- common js -->

<script src="/js/utils.js"></script>

<script src="/js/header-shrink.js"></script>

<script src="/js/back2top.js"></script>

<script src="/js/toggle-theme.js"></script>

<script src="/js/code-block.js"></script>

<script src="/js/main.js"></script>

<script src="/js/libs/anime.min.js"></script>


<!-- local search -->

    
<script src="/js/local-search.js"></script>



<!-- lazyload -->

    
<script src="/js/lazyload.js"></script>



<div class="pjax">
    <!-- home page -->
    

    <!-- post page -->
    
        <!-- post-helper -->
        
<script src="/js/post/post-helper.js"></script>


        <!-- toc -->
        
            
<script src="/js/post/toc.js"></script>

        

        <!-- copyright-info -->
        
            
<script src="/js/post/copyright-info.js"></script>

        

        <!-- share -->
        
            
<script src="/js/post/share.js"></script>

        
    

    <!-- categories page -->
    

    <!-- links page -->
    

    <!-- photos page -->
    

    <!-- tools page -->
    
</div>

<!-- mermaid -->


<!-- pjax -->

    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax'
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
        });

        document.addEventListener('pjax:send', () => {
            KEEP.utils.pjaxProgressBarStart()
        });

        document.addEventListener('pjax:complete', () => {
            KEEP.utils.pjaxProgressBarEnd()
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'))
            KEEP.initExecute()
        });
    });
</script>




</body>
</html>
